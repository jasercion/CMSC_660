\documentclass{article}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{tabu}
\usepackage{hyperref}

\makeatletter
\def\lecture{\@ifnextchar[{\@lectureWith}{\@lectureWithout}}
\def\@lectureWith[#1]{\medbreak\refstepcounter{section}%
	\renewcommand{\leftmark}{Lecture \thesection}
	\noindent{\addcontentsline{toc}{section}{Lecture \thesection: #1\@addpunct{.}}%
		\sectionfont Lecture \thesection. #1\@addpunct{.}}\medbreak}
\def\@lectureWithout{\medbreak\refstepcounter{section}%
	\renewcommand{\leftmark}{Lecture \thesection}
	\noindent{\addcontentsline{toc}{section}{Lecture \thesection.}%
		\sectionfont Lecture \thesection.}\medbreak}
\makeatother

\newcommand{\n}{\newline}

\title{Lectures on Scientific Computing}
\date{10/2/18}
\author{Lise-Marie Imbert-Gerard}
\begin{document}
	\maketitle
	\section{Software}
	\subsection{Programming for Performance}
	To program for proper performance:
	\begin{itemize}
		\item Design test 
		\item Trade offs: Speed/clarity/numerical stability
		\item Timing your code.  Matlab has a built in code profiler which can help find computational bottlenecks 
	\end{itemize}
	\section{Chapter 4: Nonlinear Equations}
	Reference for this chapter: Nocedal-Wright "Numerical Optimization" Chapter 11 (Link on Prof. I-G's Website)\n
	
	We wish to examine the problem: For $r:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ smooth, find $x_{*}\in\mathbb{R}^{n}$ such that $r(x_{*})=0$.  We say that $x_{*}$ is a solution or a root of the equation $r(x)=0$.\n
	
	\subsection{Example}
	
	\begin{align*}
		r(x)&=\begin{bmatrix}
		x_{2}^{2}-1\\
		sin(x_{1})-x_{2}
		\end{bmatrix}\\
		x_{*}&=\begin{bmatrix}
		\frac{3\pi}{2}\\
		-1
		\end{bmatrix}\\
		x_{*}&=\begin{bmatrix}
		\frac{\pi}{2}\\
		1
		\end{bmatrix}
	\end{align*}
	
	\textbf{Hypothesis:} $r$ is continuously differentiable on $mathcal{D}\in\mathbb{R}^{n}$.  $J(x)$, the Jacobian of $r$ exists and is continuous on $\mathcal{D}$, the domain. $x_{*}$ is a degenerate solution if $J(x_{*})=0$ and $r(x_{*})$.  It is non-degenerate otherwise.\n 
	
	\subsection{Local Algorithms}
	For many of these methods, we start with an iterative method and attempt to find a convergence if it exists.
	
	For local algorithms, they converge if your initial case is close enough to the actual solution (like Newton's Method).
	
	\subsubsection{Newton's Method}
	
	Given some iterate $x_{k}$, you find the value of $r(x_{k})$ and find the tangent.  Then you replace the function with its tangent meaning $x_{k+1}=$the tangent at $r(x_k)$.  
	
	\textbf{Theorem:\n}
	
	Suppose $x$ and $p$ are in $\mathcal{D}$.  Then $r(x+p)=r(x)+\int_{0}^{1}J(x+tp)pdt$.\n
	
	We define $M_{k}(p):=r(x_k)+J(x_k)p$\n
	
	\textbf{Definition:\n}
	
	Newton's Method, in its pure form, chooses step $p_{k}$ to be such that $M_{k}(p_{k})=0\rightarrow p_{k}=-J(x_{k})^{-1}r(x_{k})$\n
	
	\textbf{Algorithm:\n}
	
	Choose $x_{0}$ for $x\in\mathbb{N}$.  Calculate the solution $p_{k}$ of $J(x_{k})p_{k}=-r(x_{k})$.  Then
	\begin{align*}
		x_{k+1}=x_k+p_{k}
	\end{align*}  
	
	\textbf{Remark:} If $x_{0}$ is far from $x_{*}$, the algorithm may behave erratically.  Also note that Newton's method is dependent on the Jacobian and calculating derivatives may be complicated.  If $n$ is large this method may also be expensive since calculating $p_{k}$ will be expensive.  If $J(x_{*})$ is singular you can also run into problems.\n
	
	\textbf{1D Example:\n}
	
	\begin{align*}
		r(x)&=x^{2}\\
		J(x)&=2x\\
		\forall x_{0}&\neq0\\
		\therefore x_{k}&=\frac{1}{2^{k}}x_{0}
	\end{align*}
	
	This converges only linearly, which is a problem for Newton's method.
	
	\textbf{Theorem: \n}
	
	Let $\mathcal{D}$ be a convex open set.  Let $x_{*}\in\mathcal{D}$ be a non-degenerate solution of $r(x)=0$ and ${x_{k}}$ be the sequence generated by the Newton's Method Algorithm.  \n
	
	\begin{itemize}
		\item	When $x_{k}\in\mathcal{D}$ is sufficiently close to $x_{*}$, we have $x_{k+1}-x_{*}=0(||x_{k}-x_{*}||)$ indicating local superlinear Q Convergence
		\item When $R$ is Lipschitz continuously differentiable in the neighborhood of $x_{*}$ then for all $x_k$ sufficiently close to $x_{*}$ you will have local Q-quadratic convergence.  i.e. $x_{k+1}-x_{*}=0(||x_{k}-x_{*}||^2)$
	\end{itemize}  
	
	As a reminder, the Lipschitz hypothesis is $||J(x_{1}-J(x_0))||\leq\beta_{L}||x_{0}-x_{1}||\forall x_{1},x_{0}\in\mathcal{D}$.
	
	\textbf{Proof: \n}
	
	\begin{align*}
		r(x_k)=r(x_k)r(x_*)&=\int_{0}^{1}J(x_{k}+t(x_{*}-x_{k}))(x_{k}-x_{*})dt\\
		&=J(x_{k})(x_{k}-x_{*})+\int_{0}^{1}(J(x_{k}+t(x_{*}-x_k))-J(x_{k}))(x_{k}-x_{*})dt
	\end{align*}
	
	Let $\int_{0}^{1}(J(x_{k}+t(x_{*}-x_k))-J(x_{k}))(x_{k}-x_{*})dt=w(x_{k},x_{*})$.  We also have $||w(x_{k},x_{*})||\leq\int_{0}^{1}||J(x_{k}+t(x_{k}-x_{*}))-J(x_{k})||\cdot||x_{k}-x_{*}||dt=0(||x_{k}-x_{*}||)$ since J is continuous.
	
	\begin{align*}
		\therefore r(x_{k})=J(x_{k})(x_{k}-x_{*})+0(||x_{k}-x_{*}||)
	\end{align*}
	
	$x_{*}$ is non degenerate so $\delta>0,\beta_{*}>0$ such that $||x-x_{*}||<\delta=0\rightarrow||J(x)^{-1}||<\beta_{*}$ so $\forall x\in\beta(x_{*},\delta):={x\in\mathbb{R}^n,||x-x_{*}||\leq\delta}$.  $J(x_{k})^{-1}r(x_{k})=-p_{k}=x_{k}-x_{*}+J(x_{k})^{-1}0(||x_{k}-x_{*}||)\rightarrow x_{k}+p_{k}-x_{*}=0(||x_{k}-x_{*}||)$.  Now if $r$ is Lipschitz continuous $||w(x_{k},x_{*})\leq\int_{0}^{1}\beta_{L}||x_{k}+t(x_{k}-x_{*})-x_{k}||\cdot||x_{k}-x_{*}||dt=\beta_{L}||x_{k}-x_{*}||^{2}\int_{0}^{1}|t|dt=1$.  Therefore,
	
	\begin{align*}
		r(x_{k})&=J(x_{k})(x_{k}-x_{*})+\mathcal{O}(||x_{k}-x_{*}||^{2})\\
		\therefore\forall\in\beta(x_{*},\delta), J(x_{k})^{-1}r(x_{k})=x_{k}-x_{*}+J(x_k)^{-1}\mathcal{O}(||x_{k}-x_{*}||^{2})\rightarrow x_{k}+p_{k}-x_{*}=\mathcal{O}(||x_{k}-x_{*}||^{2})
	\end{align*}
	
\end{document}