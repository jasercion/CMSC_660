\documentclass{article}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{tabu}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{listings}
\graphicspath{ {./Images/} }

\makeatletter
\def\lecture{\@ifnextchar[{\@lectureWith}{\@lectureWithout}}
\def\@lectureWith[#1]{\medbreak\refstepcounter{section}%
	\renewcommand{\leftmark}{Lecture \thesection}
	\noindent{\addcontentsline{toc}{section}{Lecture \thesection: #1\@addpunct{.}}%
		\sectionfont Lecture \thesection. #1\@addpunct{.}}\medbreak}
\def\@lectureWithout{\medbreak\refstepcounter{section}%
	\renewcommand{\leftmark}{Lecture \thesection}
	\noindent{\addcontentsline{toc}{section}{Lecture \thesection.}%
		\sectionfont Lecture \thesection.}\medbreak}
\makeatother

\newcommand{\n}{\newline}

\title{CMSC 660 HW II}
\date{9/10/18}
\author{Joe Asercion}
\begin{document}
	\maketitle
	\section{Chapter 4, Problem 1}
		\subsection{Calculation}
		Let $L$ be the differentiation operator which takes $P_{3}$ to $P_{2}$ described in section 4.2.2.  Let $f_{k}=H_{k}(x)$ for $k=0,1,2,3$ be the Hermite Polynomial basis of $P_{3}$ and $g_{k}=H_{k}(x)$ for $k=0,1,2$ be the Hermite Polynomial basis of $P_{2}$.  What is the matrix $A$ that represents this $L$ in these bases?
		
		\subsubsection{Answer}
		
		First, we note the values of the Probabilist's Hermite Polynomials \cite{BG}(pg. 71)\cite{wiki_hermite} relevant to this problem.  Specifically:
		
		\begin{align*}
			H_{0}&=1 \\
			H_{1}&=x \\
			H_{2}&=x^{2}-1 \\ 
			H_{3}&=x^{3}-3x \\
		\end{align*}

		Therefore, we can state that the Hermite Polynomial Bases for $P_{3}$ and $P_{2}$ are
		
		\begin{center}
		\begin{tabular}{ c c }
			$P_{3}$ & $P_{2}$ \\
			\hline
			$f_{0}=1$ & $g_{0}=1$ \\
			$f_{1}=x$ & $g_{1}=x$ \\
			$f_{2}=x^{2}-1$ & $g_{2}=x^{2}-1$ \\
			$f_{3}=(x^{3}-3x)$ & 
		\end{tabular}
		\end{center}
		
		Applying the $f_{k}$ basis to $P_{3}$ yields the expanded polynomial 
		
		\begin{center}
			\begin{equation}
				P_{3}=p_{0}+p_{1}x+p_{2}(x^{2}-1)+p_{3}(x^{3}-3x)
			\end{equation}
		\end{center}
		
		This implies that 
		
		\begin{center}
			\begin{align*}
				AP_{3}&=\frac{d}{dx}(p_{0}+p_{1}x+p_{2}(x^{2}-1)+p_{3}(x^{3}-3x)) \\
				&=p_{1}+p_{2}(2x)+p_{3}(3(x^{2}-1))
			\end{align*}
		\end{center}
		
		Expressing this polynomial in terms of a vector of coefficients and the basis $P_{2}$ yields:
		
		\begin{center}
			\begin{equation}
			AP_{3}[f_{k}] = P_{2}[g_{k}]\rightarrow A
			\begin{bmatrix} p_{0}\\p_{1}\\p_{2}\\p_{3}\end{bmatrix}
			\begin{bmatrix} 1 \\ x \\ x^{2}-1 \\ x^{3}-3x \end{bmatrix}=
			\begin{bmatrix} p_{1}\\2p_{2}\\3p_{3}\end{bmatrix}
			\begin{bmatrix} 1 \\ x \\ x^{2}-1\end{bmatrix}
			\end{equation}
		\end{center}
		
		Therefore, we calculate that the matrix A must be 
		
		\begin{center}
			\begin{equation}
				A= \begin{bmatrix}0&1&0&0\\0&0&2&0\\0&0&0&3\end{bmatrix}
			\end{equation}
		\end{center}
		
		We can confirm this by applying $A$ to $P_{3}$ then applying $P_{2}$'s basis to verify that the generated polynomial is equivalent to $P_{2}$:
		
		\begin{center}
			\begin{equation}
				\begin{bmatrix}0&1&0&0\\0&0&2&0\\0&0&0&3\end{bmatrix}
				\begin{bmatrix}p_{0}\\p_{1}\\p_{2}\\p_{3}\end{bmatrix}=
				p_{1}+2p_{2}+3p_{3}
			\end{equation}
		\end{center}
		
		Vectorizing this polynomial and applying the basis for $P_{2}$ yields:
		
		\begin{center}
			\begin{equation}
				\begin{bmatrix}p_{1}\\2p_{2}\\3p_{3}\end{bmatrix}
				\begin{bmatrix}1\\x\\x^{2}-1\end{bmatrix}=
				p_{1}+2xp_{2}+3(x^{2}-1)p_{3} = \frac{d}{dx}(p_{0}+p_{1}x+p_{2}(x^{2}-1)+p_{3}(x^{3}-3x))
			\end{equation}
		\end{center}
	\newpage
	\section{Chapter 4, Problem 2}
		Using the following information from the problem:\n
		\begin{itemize}
			\item $L$ is a linear transformation from $V\rightarrow V$ 
			\item $f_{1},...,f_{n}$ and $g_{1},...,g_{n}$ are to bases of $V$
			\item Any $u\in V$ can be written in a unique was as $u=\sum_{k=1}^{n}v_{k}f_{k}$ or as $u=\sum_{k=1}^{n}w_{k}g_{k}$
			\item $R$ is an $n\times n$ matrix that relates the $f_{k}$ expansion coefficients $v_{k}$ to the $g_{k}$ coefficients $w_{k}$ by $v_{j}=\sum_{k=1}^{n}r_{jk}w_{k}$ 
			\item $v=\begin{bmatrix}v_{1}\\.\\.\\.\\v_{k}\end{bmatrix}$ and $w=\begin{bmatrix}w_{1}\\.\\.\\.\\w_{k}\end{bmatrix}$ are related by $v=Rw$
			\item $A$ represents $L$ in the $f_{k}$ basis and $B$ represents $L$ in the $g_{k}$ basis
		\end{itemize}
		\subsection{a. Proof}
		Show that $B=R^{-1}AR$.
		
		\subsubsection{Answer}
		
		Because $R$ is an invertible matrix (as evidenced by the problem statement) we can use the property $AA^{-1}=I$ to rewrite the function in a way that eliminates the inversion:
		
		\begin{align*}
			B&=R^{-1}AR\\
			RB&=RR^{-1}AR\\
		\end{align*}
		\begin{equation}
		RB=AR
		\end{equation}
		
		Multiplying both sides of the expression by vector $w=[u]_{\mathcal{G}}$ yields:
		
		\begin{equation}
			RBw=ARw
		\end{equation}

		Examining the right side of equation (7), we note that we can apply the relation $v=Rw$:
		
		\begin{align*}
			ARw&=A(Rw) \\
			&=Av
		\end{align*}
		
		Examining the left side of equation (7), we note that, because $R$ directly relates coefficients in the $w$ basis to the $v$ basis,
		
		\begin{align*}
			RBw&=R(Bw)\\
			&=Av
		\end{align*}
		
		We can now see that both sides of equation (7) can be expressed as $Av$, therefore, $B=R^{-1}AR$.
		\subsection{b. Calculation}
		For $V=P_{3}$, $f_{k}=x^{k}$ and $g_{k}=H_{k}$ find R.
		
		\subsubsection{Answer}
		
		In this case, we are examining an arbitrary vector in $V$, expressed as $\begin{bmatrix} p_{0}\\p_{1}\\p_{2}\\p_{3}\end{bmatrix}$, in two different bases:
		
		\begin{align*}
			\begin{bmatrix}p_{0}\\p_{1}x\\p_{2}x^{2}\\p_{3}x^{3}\end{bmatrix}(\text{in the f basis})\\
			\&\\
			\begin{bmatrix}p_{0}\\p_{1}x\\p_{2}(x^{2}-1)\\p_{3}(x^{3}-3x)\end{bmatrix}(\text{in the g basis})
		\end{align*}
		
		We are given the relation that $v=Rw$ from the problem, therefore
		
		\begin{align*}
			\begin{bmatrix}p_{0}\\p_{1}x\\p_{2}x^{2}\\p_{3}x^{3}\end{bmatrix}&=R\begin{bmatrix}p_{0}\\p_{1}x\\p_{2}(x^{2}-1)\\p_{3}(x^{3}-3x)\end{bmatrix}
		\end{align*}
		
		Which allows us to calculate that $R$ must be 
		
		\begin{align*}
			R=
			\begin{bmatrix}
				1 & 0 & 0 & 0\\
				0 & 1 & 0 & 0\\
				1 & 0 & 1 & 0 \\
				0 & 3 & 0 & 1
			\end{bmatrix}
		\end{align*}
		
		\subsection{c. Calculation}
		
		If $L$ is the linear transformation $Lp=q$ with $q(x)=\delta_{x}(xp(x))$ find the matrix $A$ that represents $L$ in the monomial basis $f_{k}$.
		
		\subsubsection{Answer}
		
		Inserting the given values into the given transformation expression yields
		
		\begin{align*}
			A(p_{0}+p_{1}x+p_{2}x^{2}+p_{3}x^{3})&=\delta_{x}(xp(x))\\
			A(p_{0}+p_{1}x+p_{2}x^{2}+p_{3}x^{3})&=\delta_{x}(p_{0}x+p_{1}x^2+p_{2}x^{3}+p_{3}x^{4})\\
			A(p_{0}+p_{1}x+p_{2}x^{2}+p_{3}x^{3})&=p_{0}+2p_{1}x+3p_{2}x^2+4p_{3}x^{3}
		\end{align*}
		
		Therefore we calculate that the matrix $A$ must be 
		\begin{align*}
		A=
		\begin{bmatrix}
			1 & 0 & 0 & 0\\
			0 & 2 & 0 & 0\\
			0 & 0 & 3 & 0\\
			0 & 0 & 0 & 4
		\end{bmatrix}
		\end{align*}
		\subsection{d. Calculation}
		
		Find the matrix $B$ that represents $L$ in the Hermite polynomial basis $H_{k}$
				
		\subsubsection{Answer}
		We perform the same steps as in part c:
		
		\begin{align*}
		B(p_{0}+p_{1}x+p_{2}(x^{2}-1)+p_{3}(x^{3}-3x))&=\delta_{x}(xp(x))\\
		B(p_{0}+p_{1}x+p_{2}(x^{2}-1)+p_{3}(x^{3}-3x))&=\delta_{x}(p_{0}x+p_{1}x^2+p_{2}(x^{3}-x)+p_{3}(x^{4}-3x^2))\\
		B(p_{0}+p_{1}x+p_{2}(x^{2}-1)+p_{3}(x^{3}-3x))&=p_{0}+2p_{1}x+p_{2}(3x^2-1)+p_{3}(4x^{3}-6x)
		\end{align*}
		
		Therefore we calculate that the matrix $B$ must be 
		\begin{align*}
		B=
		\begin{bmatrix}		
		1 & 0 & 0 & 0 \\
		0 & 2 & 0 & 0 \\
		2 & 0 & 3 & 0\\
		0 & 6 & 0 & 4
		\end{bmatrix}
		\end{align*}
		
		\subsection{e. Calculation}
		
		Multiply the matrices to check explicitly that $B=R^{-1}AR$ in this case.
		
		\subsubsection{Answer}
		
		\begin{align*}
		R^{-1}&=
		\begin{bmatrix}
		1 & 0 & 0 & 0\\
		0 & 1 & 0 & 0\\
		-1 & 0 & 1 & 0 \\
		0 &-3 & 0 & 1
		\end{bmatrix}\\
		\therefore R^{-1}AR &=
		\begin{bmatrix}
		1 & 0 & 0 & 0\\
		0 & 1 & 0 & 0\\
		-1 & 0 & 1 & 0 \\
		0 &-3 & 0 & 1
		\end{bmatrix}
		\begin{bmatrix}
		1 & 0 & 0 & 0\\
		0 & 2 & 0 & 0\\
		0 & 0 & 3 & 0\\
		0 & 0 & 0 & 4
		\end{bmatrix}
		\begin{bmatrix}
		1 & 0 & 0 & 0\\
		0 & 1 & 0 & 0\\
		1 & 0 & 1 & 0 \\
		0 &3 & 0 & 1
		\end{bmatrix}\\
		&=\begin{bmatrix}
		1 & 0 & 0 & 0\\
		0 & 2 & 0 & 0 \\
		2 & 0 & 3 & 0 \\
		0 & 6 & 0 & 4
		\end{bmatrix}		
		\end{align*}
		
		Which is equal to our calculated value for matrix $B$.
		
	\section{Chapter 4, Problem 3}
		\subsection{Proof}
		Let $A$ be an $n\times m$ matrix and $B$ be an $m\times l$ matrix.  Then $AB$ is an $n\times l$ matrix.  Show that 
		
		\begin{equation}
			(AB)^*=B^*A^*
		\end{equation}
		
		\subsubsection{Answer}
		
		To find the adjoint of a matrix we first take the complex conjugate of each element then take the transpose of the resulting matrix.\cite{BG}(pg. 74)
		
		Starting with the left hand side of equation (8), we calculate the general form of matrix $C:=AB$:
		
		\begin{align*}
		C=
			\begin{bmatrix}
			\sum_{k=1}^{m}a_{1,k}b_{k,1}&...&\sum_{k=1}^{m}a_{1,k}b_{k,l}\\
			...&...&...\\
			sum_{k=1}^{m}a_{n,k}b_{k,1}&...&\sum_{k=1}^{m}a_{n,k}b_{k,l}\\
			\end{bmatrix}
		\end{align*}
		
		Therefore,
		
		\begin{align*}
			C^{*}&=\Bigg(
				\begin{bmatrix}
				\overline{\sum_{k=1}^{m}a_{1,k}b_{k,1}}&...&\overline{\sum_{k=1}^{m}a_{1,k}b_{k,l}}\\
				...&...&...\\
				\overline{\sum_{k=1}^{m}a_{n,k}b_{k,1}}&...&\overline{\sum_{k=1}^{m}a_{n,k}b_{k,l}}\\
				\end{bmatrix}\Bigg)^{T}\\
				&=
				\begin{bmatrix}
				\overline{\sum_{k=1}^{m}a_{1,k}b_{k,1}}&...&\overline{\sum_{k=1}^{m}a_{n,k}b_{k,1}}\\
				...&...&...\\
				\overline{\sum_{k=1}^{m}a_{1,k}b_{k,l}}&...&\overline{\sum_{k=1}^{m}a_{n,k}b_{k,l}}\\
				\end{bmatrix}
		\end{align*}
		
		Examining the right hand side of equation (8), 
		
		\begin{align*}
			B^*&=
			\Bigg(
			\begin{bmatrix}
			\overline{b_{1,1}}&...&\overline{b_{1,l}}\\
			...&...&...\\
			\overline{b_{m,1}}&...&\overline{b_{m,l}}
			\end{bmatrix}
			\Bigg)^{T}\\
			&=
			\begin{bmatrix}
			\overline{b_{1,1}}&...&\overline{b_{m,1}}\\
			...&...&...\\
			\overline{b_{1,l}}&...&\overline{b_{m,l}}
			\end{bmatrix}
		\end{align*}

		\begin{align*}
		A^*&=
		\Bigg(
		\begin{bmatrix}
		\overline{a_{1,1}}&...&\overline{a_{1,m}}\\
		...&...&...\\
		\overline{a_{n,1}}&...&\overline{a_{n,m}}
		\end{bmatrix}
		\Bigg)^{T}\\
		&=
		\begin{bmatrix}
		\overline{a_{1,1}}&...&\overline{a_{n,1}}\\
		...&...&...\\
		\overline{a_{1,m}}&...&\overline{a_{n,l}}
		\end{bmatrix}
		\end{align*}		
		
		\begin{align*}
			\therefore B^*A^*=
			\begin{bmatrix}
			\sum_{k=1}^{m}\overline{a_{1,k}}\overline{b_{k,1}}&...&\sum_{k=1}^{m}\overline{a_{n,k}}\overline{b_{k,1}}\\
			...&...&...\\
			\sum_{k=1}^{m}\overline{a_{1,k}}\overline{b_{k,l}}&...&\sum_{k=1}^{m}\overline{a_{n,k}}\overline{b_{k,l}}
			\end{bmatrix}
		\end{align*}
		
		Because $\overline{ab}=\overline{a}\overline{b}$, we see that, in fact, $(AB)^{*}=A^{*}B^{*}$.
	\section{Chapter 4: Questions}
		\subsection{Definition}
		Define a 'real vector space'.
		
		\subsubsection{Answer}
		As noted in B\&G, a real vector space denoted by $\mathbb{R}^{n}$ consists of one or more column vectors with $n$ components: $u=\begin{bmatrix}
		u_{0}\\
		.\\
		.\\
		.\\
		u_{n}
		\end{bmatrix}$ with each arbitrary $u_{n}\in\mathbb{R}$.  
		As with Complex Vector Spaces, these vectors can be scaled or added component-wise \cite{BG}(pg.69).  
		
		
		\subsection{Proof}
		Is $V':={u\in\mathbb{R}^{n},\sum_{k=1}^{n}u_{k}=0}$ a subspace of $\mathbb{R}^{n}$?  Provide a proof of your assertion.	
		
		\subsubsection{Answer}
		
		Yes.  As noted in \cite{BG}(pg.69), this vector space is closed under vector addition or scalar multiplication and is therefore a subspace of $\mathbb{R}^{n}$.
		
		\subsection{Proof}
		Is $V':={u\in\mathbb{R}^{n},\sum_{k=1}^{n}u_{k}=1}$ a subspace of $\mathbb{R}^{n}$?  Provide a proof of your assertion.	
			
		\subsubsection{Answer}
		No.  As noted in \cite{BG}(pg. 69) this vector space is not closed under vector addition or scalar multiplication and is therefore not a subspace of $\mathbb{R}^{n}$.  This can be proved by noting that the by the definition of the subspace the addition of any two vectors within the subspace must equal 2.  This is outside the vector space, therefore  $V':={u\in\mathbb{R}^{n},\sum_{k=1}^{n}u_{k}=1}$ is not closed under addition.
		
		\subsection{Definition}
		What is the standard basis of $\mathbb{R}^{n}$?
		
		\subsubsection{Answer}
		The standard basis of $\mathbb{R}^{n}$ expressed as an $n\times n$ matrix is $u=\begin{bmatrix}
		1 & 0 & . & . & . & u_{0,n}=0\\
		0 & 1 & 0 & . & . & 0\\
		0 & 0 & 1 & 0 & . & 0\\
		0 & . & 0 & 1 & 0 & 0 \\
		. & . & . & . & . & . \\
		. & . & . & . & . & . \\
		. & . & . & . & . & . \\
		u_{n,0}=0 & 0 & . & . & 0 & u_{n,n}=1
		\end{bmatrix}$
		
		i.e. a diagonal matrix where all diagonal components are equal to one and all off diagonal components are equal to zero.\cite{BG}(pg. 70)
		\subsection{Definition}
		What is the standard inner product of $\mathbb{R}^{n}$?
		
		\subsubsection{Answer}
		From equation (4.2)\cite{BG}, the standard inner product of two vectors in $\mathbb{R}^{n}$ is 
		
		\begin{align*}
			<u,v>=\sum_{k=1}^{n}u_{k}v_{k}
		\end{align*}
		
		
		\subsection{Proof}
		Prove that the space of polynomials with real coefficients is a vector space.
		
		\subsubsection{Answer}
		Because $\mathbb{R}$ is closed under multiplication and addition, the space of polynomials with coefficients$\in\mathbb{R}$ must also be closed.  Therefore this set can be considered a vector space.
		
		\subsection{Concept}
		If $A$ is a real matrix, what is the difference between its transpose and its adjoint?
		
		\subsubsection{Answer}
		There is no difference.  As defined in \cite{BG} (pg.74), the adjoint of a matrix is the transpose of the complex conjugate of its components.  For matrices where every element $\in\mathbb{R}^{n}$, there is no imaginary component of the element.  Therefore, $A^{T}=A^{*},\text{ }A\in\mathbb{R}^{n}$
		
\begin{thebibliography}{2}
	\bibitem{BG}
	David Bindel and Johnathan Goodman.
	\textit{Principles of Scientific Computing}. 
	2009.
	\bibitem{wiki_hermite}
	Hermite Polynomials Wikipedia Page
	\\\texttt{https://en.wikipedia.org/wiki/Hermite\_polynomials}
\end{thebibliography}
\end{document}